{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 – Evaluation & Analysis\n",
    "\n",
    "Loads the trained models, makes predictions on the held-out test set, computes **accuracy, precision, recall, F1, MCC**, draws confusion matrices, bar-plots and saves a final `results.pkl` + CSV summary.\n",
    "\n",
    "**Inputs**  \n",
    "- `qvc_model.pkl`  \n",
    "- `qsvm_model.pkl`  \n",
    "- `preprocessing_objects.pkl`  \n",
    "- `X_selected.csv` & `y.csv` (for test split)\n",
    "\n",
    "**Outputs**  \n",
    "- `model_comparison.csv`  \n",
    "- `results.pkl`  \n",
    "- PNG visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas numpy scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, pickle, matplotlib.pyplot as plt, seaborn as sns\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, matthews_corrcoef, confusion_matrix,\n",
    "                             classification_report)\n",
    "from datetime import datetime\n",
    "print(f\"Start: {datetime.now():%Y-%m-%d %H:%M:%S}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 1. Load artefacts"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/content/drive/MyDrive/QuantumBoost2025/'\n",
    "with open(base+'qvc_model.pkl','rb') as f:   qvc = pickle.load(f)\n",
    "with open(base+'qsvm_model.pkl','rb') as f:  qsvm = pickle.load(f)\n",
    "with open(base+'preprocessing_objects.pkl','rb') as f: prep = pickle.load(f)\n",
    "\n",
    "X = pd.read_csv(base+'X_selected.csv')\n",
    "y = pd.read_csv(base+'y.csv').squeeze()\n",
    "\n",
    "# Re-create test set (same split as training notebook)\n",
    "from sklearn.model_selection import train_test_split\n",
    "_, X_test, _, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Apply same preprocessing pipeline\n",
    "X_test_r = prep['robust'].transform(X_test)\n",
    "X_test_q = prep['minmax'].transform(X_test_r)\n",
    "X_test_pca = prep['pca'].transform(X_test_q)\n",
    "X_test_pca = prep['pca_scaler'].transform(X_test_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 2. Predictions"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_qvc  = qvc.predict(X_test_pca)\n",
    "y_pred_qsvm = qsvm.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 3. Metrics helper"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_true, y_pred, name):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'mcc': matthews_corrcoef(y_true, y_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 4. Compute & display"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qvc_met  = metrics(y_test, y_pred_qvc,  \"QVC\")\n",
    "qsvm_met = metrics(y_test, y_pred_qsvm, \"QSVM\")\n",
    "\n",
    "comp = pd.DataFrame([['QVC']+list(qvc_met.values()),\n",
    "                     ['QSVM']+list(qsvm_met.values())],\n",
    "                    columns=['Model','Accuracy','Precision','Recall','F1','MCC'])\n",
    "print(comp)\n",
    "\n",
    "best = comp.loc[comp['MCC'].idxmax(),'Model']\n",
    "print(f\"\\nBest model (MCC): {best}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 5. Visualisations"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot\n",
    "fig, ax = plt.subplots(1,5,figsize=(20,4))\n",
    "for i, col in enumerate(comp.columns[1:]):\n",
    "    comp.plot(x='Model', y=col, kind='bar', ax=ax[i], legend=False, color=['#1f77b4','#ff7f0e'])\n",
    "    ax[i].set_ylim(0,1)\n",
    "    ax[i].set_title(col)\n",
    "plt.tight_layout()\n",
    "plt.savefig(base+'model_comparison.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrices\n",
    "cm_qvc  = confusion_matrix(y_test, y_pred_qvc)\n",
    "cm_qsvm = confusion_matrix(y_test, y_pred_qsvm)\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(10,4))\n",
    "sns.heatmap(cm_qvc, annot=True, fmt='d', cmap='Blues', ax=ax[0])\n",
    "ax[0].set_title('QVC')\n",
    "sns.heatmap(cm_qsvm, annot=True, fmt='d', cmap='Oranges', ax=ax[1])\n",
    "ax[1].set_title('QSVM')\n",
    "plt.savefig(base+'confusion_matrices.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 6. Save final results"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp.to_csv(base+'model_comparison.csv', index=False)\n",
    "with open(base+'results.pkl','wb') as f:\n",
    "    pickle.dump({'qvc':qvc_met, 'qsvm':qsvm_met, 'comparison':comp, 'best':best}, f)\n",
    "print(\"Evaluation complete – all files saved.\")"
   ]
  }
 ],
 "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3"}},
 "nbformat": 4,
 "nbformat_minor": 2
}